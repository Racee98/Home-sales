{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d0d6bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14960\\1749719005.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import findspark and initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "# Import findspark and initialize. \n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b991794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a64e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a temporary view of the DataFrame.\n",
    "df.createOrReplaceTempView('homesales')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
    "spark.sql(\"SELECT year_sold, round(avg(price), 2) Avg_Price FROM homesales WHERE bedrooms == 4 GROUP BY 1 ORDER BY 1 ASC\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cda2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal placesspark.sql(\"SELECT date_built, round(avg(price),2) Avg_Price_3bd_3bath FROM homesales WHERE bedrooms == 3 AND bathrooms == 3 GROUP BY 1 ORDER BY 1 ASC\").show()\n",
    "spark.sql(\"SELECT date_built, round(avg(price),2) Avg_Price_3bd_3bath FROM homesales WHERE bedrooms == 3 AND bathrooms == 3 GROUP BY 1 ORDER BY 1 ASC\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4908607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
    "spark.sql(\"SELECT date_built, round(avg(price), 2) Avg_Big_House_Price FROM homesales WHERE bedrooms ==3 AND bathrooms==3 AND floors ==2 AND sqft_living > 2000 GROUP BY 1 ORDER BY 1 ASC\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54207304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than\n",
    "# or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
    "window = Window.orderBy(df['view'].desc())\n",
    "\n",
    "df_result = df.groupBy(\"view\") \\\n",
    "              .agg(round(avg(\"price\"), 2).alias(\"Avg_Price\")) \\\n",
    "              .withColumn(\"rn\", row_number().over(window)) \\\n",
    "              .filter(col(\"rn\") >= 350000) \\\n",
    "              .orderBy(\"view\", ascending=False)\n",
    "\n",
    "df_result.show()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Cache the the temporary table home_sales.\n",
    "df.createOrReplaceTempView(\"homesales\")\n",
    "df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Check if the table is cached.\n",
    "spark.catalog.isCached('home_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ea45bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Using the cached data, run the query that filters out the view ratings with average price \n",
    "#  greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
    "df_result = df.groupBy(\"view\") \\\n",
    "              .agg(round(avg(\"price\"), 2).alias(\"Avg_Price\")) \\\n",
    "              .filter(col(\"Avg_Price\") >= 350000) \\\n",
    "              .orderBy(\"view\", ascending=False)\n",
    "\n",
    "df_result.show()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738cdba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data \n",
    "df.write.partitionBy('date_built').mode('overwrite').parquet('homesales_partitioned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36945048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Read the parquet formatted data.\n",
    "part_df = spark.read.format('parquet').load('homesales_partitioned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3367f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Create a temporary table for the parquet data.\n",
    "part_df.createOrReplaceTempView('p_homesales')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc13604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Run the query that filters out the view ratings with average price of greater than or equal to $350,000 \n",
    "# with the parquet DataFrame. Round your average to two decimal places. \n",
    "# Determine the runtime and compare it to the cached version.\n",
    "\n",
    "start_time = time.time()\n",
    "spark.sql(\"\"\"SELECT view, round(avg(price), 2) Avg_Price\n",
    "FROM p_homesales\n",
    "GROUP BY 1\n",
    "HAVING avg(price) >= 350000\n",
    "ORDER BY 1 DESC\"\"\").show()\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291064a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Uncache the home_sales temporary table.\n",
    "spark.sql(\"uncache table homesales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53925915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Check if the home_sales is no longer cached\n",
    "spark.catalog.isCached('homesales')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
